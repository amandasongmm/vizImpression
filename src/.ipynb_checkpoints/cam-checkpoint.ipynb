{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 1.15.4\n",
      "Pandas version: 0.23.4\n",
      "scikit-image version: 0.14.1\n",
      "TensorFlow version: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle as Pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import time\n",
    "import numpy as np\n",
    "print ('Numpy version: ' + np.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "print ('Pandas version: ' + pd.__version__)\n",
    "\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "print ('scikit-image version: ' + skimage.__version__)\n",
    "\n",
    "import tensorflow as tf\n",
    "print ('TensorFlow version: ' + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/Users/chenguanghao/Desktop/Amanda/Weakly_detector'\n",
    "dataset_path = os.path.join(root_path,'datasets/256_ObjectCategories')\n",
    "weight_path = os.path.join(root_path,'trained_models/VGG/caffe_layers_value.pickle')\n",
    "model_path = os.path.join(root_path,'trained_models/VGG/')\n",
    "saved_model_name = 'model-1'\n",
    "# This model is used/needed for testing (e.g., one of the above models):\n",
    "saved_model_name_testing = 'model-1-1'\n",
    "\n",
    "# The following parameters are optional:\n",
    "# Loading a pre-trained model before starting the training, if it exists.\n",
    "pretrained_model = None # None or '/workspace/trained_models/VGG/model-1-1'\n",
    "# These are created from dataset, if they are not already on disk:\n",
    "trainset_path = os.path.join(root_path,'datasets/train.pickle')\n",
    "testset_path = os.path.join(root_path,'datasets/test.pickle')\n",
    "label_dict_path = os.path.join(root_path,'datasets/label_dict.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image( path ):\n",
    "    try:\n",
    "        img = skimage.io.imread( path ).astype( float )\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    if img is None: return None\n",
    "    if len(img.shape) < 2: return None\n",
    "    if len(img.shape) == 4: return None\n",
    "    if len(img.shape) == 2: img=np.tile(img[:,:,None], 3)\n",
    "    if img.shape[2] == 4: img=img[:,:,:3]\n",
    "    if img.shape[2] > 4: return None\n",
    "\n",
    "    img /= 255.\n",
    "\n",
    "    short_edge = min( img.shape[:2] )\n",
    "    yy = int((img.shape[0] - short_edge) / 2)\n",
    "    xx = int((img.shape[1] - short_edge) / 2)\n",
    "    crop_img = img[yy:yy+short_edge, xx:xx+short_edge]\n",
    "    resized_img = skimage.transform.resize( crop_img, [224,224] , mode='constant')     #resize the image here\n",
    "    '''\n",
    "    #OldScikitImage\n",
    "    resized_img = skimage.transform.resize( crop_img, [224,224] )\n",
    "    '''\n",
    "    return resized_img\n",
    "\n",
    "class StrToBytes:\n",
    "    def __init__(self, fileobj):\n",
    "        self.fileobj = fileobj\n",
    "    def read(self, size):\n",
    "        return self.fileobj.read(size).encode()\n",
    "    def readline(self, size=-1):\n",
    "        return self.fileobj.readline(size).encode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Class Activation Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector():\n",
    "    def __init__(self, weight_file_path, n_labels):\n",
    "        self.image_mean = [103.939, 116.779, 123.68]\n",
    "        self.n_labels = n_labels\n",
    "\n",
    "        with open(weight_file_path,'rb') as f:\n",
    "            self.pretrained_weights = Pickle.load(f,encoding='iso-8859-1')\n",
    "\n",
    "    def get_weight( self, layer_name):\n",
    "        layer = self.pretrained_weights[layer_name]\n",
    "        return layer[0]\n",
    "\n",
    "    def get_bias( self, layer_name ):\n",
    "        layer = self.pretrained_weights[layer_name]\n",
    "        return layer[1]\n",
    "\n",
    "    def get_conv_weight( self, name ):\n",
    "        f = self.get_weight( name )\n",
    "        return f.transpose(( 2,3,1,0 ))\n",
    "\n",
    "    def conv_layer( self, bottom, name ):\n",
    "        with tf.variable_scope(name) as scope:\n",
    "\n",
    "            w = self.get_conv_weight(name)\n",
    "            b = self.get_bias(name)\n",
    "\n",
    "            conv_weights = tf.get_variable(\n",
    "                    \"W\",\n",
    "                    shape=w.shape,\n",
    "                    initializer=tf.constant_initializer(w)\n",
    "                    )\n",
    "            conv_biases = tf.get_variable(\n",
    "                    \"b\",\n",
    "                    shape=b.shape,\n",
    "                    initializer=tf.constant_initializer(b)\n",
    "                    )\n",
    "\n",
    "            conv = tf.nn.conv2d( bottom, conv_weights, [1,1,1,1], padding='SAME')\n",
    "            bias = tf.nn.bias_add( conv, conv_biases )\n",
    "            relu = tf.nn.relu( bias, name=name )\n",
    "\n",
    "        return relu\n",
    "\n",
    "    def new_conv_layer( self, bottom, filter_shape, name ):\n",
    "        with tf.variable_scope( name ) as scope:\n",
    "            w = tf.get_variable(\n",
    "                    \"W\",\n",
    "                    shape=filter_shape,\n",
    "                    initializer=tf.random_normal_initializer(0., 0.01))\n",
    "            b = tf.get_variable(\n",
    "                    \"b\",\n",
    "                    shape=filter_shape[-1],\n",
    "                    initializer=tf.constant_initializer(0.))\n",
    "\n",
    "            conv = tf.nn.conv2d( bottom, w, [1,1,1,1], padding='SAME')\n",
    "            bias = tf.nn.bias_add(conv, b)\n",
    "\n",
    "        return bias #relu\n",
    "\n",
    "    def fc_layer(self, bottom, name, create=False):\n",
    "        shape = bottom.get_shape().as_list()\n",
    "        dim = np.prod( shape[1:] )\n",
    "        x = tf.reshape(bottom, [-1, dim])\n",
    "\n",
    "        cw = self.get_weight(name)\n",
    "        b = self.get_bias(name)\n",
    "\n",
    "        if name == \"fc6\":\n",
    "            cw = cw.reshape((4096, 512, 7,7))\n",
    "            cw = cw.transpose((2,3,1,0))\n",
    "            cw = cw.reshape((25088,4096))\n",
    "        else:\n",
    "            cw = cw.transpose((1,0))\n",
    "\n",
    "        with tf.variable_scope(name) as scope:\n",
    "            cw = tf.get_variable(\n",
    "                    \"W\",\n",
    "                    shape=cw.shape,\n",
    "                    initializer=tf.constant_initializer(cw))\n",
    "            b = tf.get_variable(\n",
    "                    \"b\",\n",
    "                    shape=b.shape,\n",
    "                    initializer=tf.constant_initializer(b))\n",
    "\n",
    "            fc = tf.nn.bias_add( tf.matmul( x, cw ), b, name=scope)\n",
    "\n",
    "        return fc\n",
    "\n",
    "    def new_fc_layer( self, bottom, input_size, output_size, name ):\n",
    "        shape = bottom.get_shape().to_list()\n",
    "        dim = np.prod( shape[1:] )\n",
    "        x = tf.reshape( bottom, [-1, dim])\n",
    "\n",
    "        with tf.variable_scope(name) as scope:\n",
    "            w = tf.get_variable(\n",
    "                    \"W\",\n",
    "                    shape=[input_size, output_size],\n",
    "                    initializer=tf.random_normal_initializer(0., 0.01))\n",
    "            b = tf.get_variable(\n",
    "                    \"b\",\n",
    "                    shape=[output_size],\n",
    "                    initializer=tf.constant_initializer(0.))\n",
    "            fc = tf.nn.bias_add( tf.matmul(x, w), b, name=scope)\n",
    "\n",
    "        return fc\n",
    "\n",
    "    def inference( self, rgb, train=False ):\n",
    "        rgb *= 255.\n",
    "        \n",
    "        r, g, b = tf.split(rgb, num_or_size_splits=3, axis=3)\n",
    "        bgr = tf.concat(\n",
    "            [\n",
    "                b-self.image_mean[0],\n",
    "                g-self.image_mean[1],\n",
    "                r-self.image_mean[2]\n",
    "            ], axis=3)\n",
    "        '''\n",
    "        #OldTF\n",
    "        r, g, b = tf.split(3, 3, rgb)\n",
    "        bgr = tf.concat(3,\n",
    "            [\n",
    "                b-self.image_mean[0],\n",
    "                g-self.image_mean[1],\n",
    "                r-self.image_mean[2]\n",
    "            ])\n",
    "        '''\n",
    "\n",
    "        relu1_1 = self.conv_layer( bgr, \"conv1_1\" )\n",
    "        relu1_2 = self.conv_layer( relu1_1, \"conv1_2\" )\n",
    "\n",
    "        pool1 = tf.nn.max_pool(relu1_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                                         padding='SAME', name='pool1')\n",
    "\n",
    "        relu2_1 = self.conv_layer(pool1, \"conv2_1\")\n",
    "        relu2_2 = self.conv_layer(relu2_1, \"conv2_2\")\n",
    "        pool2 = tf.nn.max_pool(relu2_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                               padding='SAME', name='pool2')\n",
    "\n",
    "        relu3_1 = self.conv_layer( pool2, \"conv3_1\")\n",
    "        relu3_2 = self.conv_layer( relu3_1, \"conv3_2\")\n",
    "        relu3_3 = self.conv_layer( relu3_2, \"conv3_3\")\n",
    "        pool3 = tf.nn.max_pool(relu3_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                               padding='SAME', name='pool3')\n",
    "\n",
    "        relu4_1 = self.conv_layer( pool3, \"conv4_1\")\n",
    "        relu4_2 = self.conv_layer( relu4_1, \"conv4_2\")\n",
    "        relu4_3 = self.conv_layer( relu4_2, \"conv4_3\")\n",
    "        pool4 = tf.nn.max_pool(relu4_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                               padding='SAME', name='pool4')\n",
    "\n",
    "        relu5_1 = self.conv_layer( pool4, \"conv5_1\")\n",
    "        relu5_2 = self.conv_layer( relu5_1, \"conv5_2\")\n",
    "        relu5_3 = self.conv_layer( relu5_2, \"conv5_3\")\n",
    "\n",
    "        conv6 = self.new_conv_layer( relu5_3, [3,3,512,1024], \"conv6\")\n",
    "        gap = tf.reduce_mean( conv6, [1,2] )\n",
    "\n",
    "        with tf.variable_scope(\"GAP\"):\n",
    "            gap_w = tf.get_variable(\n",
    "                    \"W\",\n",
    "                    shape=[1024, self.n_labels],\n",
    "                    initializer=tf.random_normal_initializer(0., 0.01))\n",
    "\n",
    "        output = tf.matmul( gap, gap_w)\n",
    "\n",
    "        return pool1, pool2, pool3, pool4, relu5_3, conv6, gap, output\n",
    "\n",
    "    def get_classmap(self, label, conv6):\n",
    "        conv6_resized = tf.image.resize_bilinear( conv6, [224, 224] )\n",
    "        with tf.variable_scope(\"GAP\", reuse=True):\n",
    "            label_w = tf.gather(tf.transpose(tf.get_variable(\"W\")), label)\n",
    "            label_w = tf.reshape( label_w, [-1, 1024, 1] ) # [batch_size, 1024, 1]\n",
    "\n",
    "        conv6_resized = tf.reshape(conv6_resized, [-1, 224*224, 1024]) # [batch_size, 224*224, 1024]\n",
    "\n",
    "        classmap = tf.matmul( conv6_resized, label_w )\n",
    "        '''\n",
    "        #OldTF\n",
    "        classmap = tf.batch_matmul( conv6_resized, label_w )\n",
    "        '''\n",
    "        \n",
    "        classmap = tf.reshape( classmap, [-1, 224,224] )\n",
    "        return classmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read from disk: trainset, testset, and label_dict\n",
      "Train set size: 28038\n",
      "Test set size: 2569\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>137</td>\n",
       "      <td>mattress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28009</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28010</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28011</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28012</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28013</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28014</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28015</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28016</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28017</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28018</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28019</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28020</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28021</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28022</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28023</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28024</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28025</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28026</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28027</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28028</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28029</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28030</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28031</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28032</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28033</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28034</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28035</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28036</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28037</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28038</th>\n",
       "      <td>/Users/chenguanghao/Desktop/Amanda/Weakly_dete...</td>\n",
       "      <td>86</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28038 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              image_path  label label_name\n",
       "0      /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "1      /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "2      /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "3      /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "4      /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "5      /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "6      /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "7      /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "8      /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "9      /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "10     /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "11     /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "12     /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "13     /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "14     /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "15     /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "16     /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "17     /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "18     /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "19     /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "20     /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "21     /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "22     /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "23     /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "24     /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "25     /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "26     /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "27     /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "28     /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "29     /Users/chenguanghao/Desktop/Amanda/Weakly_dete...    137   mattress\n",
       "...                                                  ...    ...        ...\n",
       "28009  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28010  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28011  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28012  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28013  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28014  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28015  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28016  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28017  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28018  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28019  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28020  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28021  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28022  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28023  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28024  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28025  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28026  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28027  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28028  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28029  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28030  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28031  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28032  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28033  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28034  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28035  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28036  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28037  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "28038  /Users/chenguanghao/Desktop/Amanda/Weakly_dete...     86   goldfish\n",
       "\n",
       "[28038 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists( trainset_path ):\n",
    "    print ('Starting to process the dataset and building the train, test, and label sets (only paths to the images) ...')\n",
    "    image_dir_list = os.listdir( dataset_path )\n",
    "    label_pairs = list(map(lambda x: x.split('.'), image_dir_list))\n",
    "    del_index = 0\n",
    "        \n",
    "    labels, label_names = zip(*label_pairs)\n",
    "    labels = list(map(lambda x: int(x), labels))\n",
    "\n",
    "    label_dict = pd.Series( labels, index=label_names )\n",
    "    label_dict -= 1     #make label 0-base\n",
    "    image_paths_per_label_map = map(lambda one_dir:map(lambda one_file: os.path.join(dataset_path, one_dir, one_file),\n",
    "                                                    os.listdir(os.path.join(dataset_path,one_dir))), image_dir_list)\n",
    "    image_paths_per_label = []\n",
    "    for i in list(image_paths_per_label_map):\n",
    "        image_paths_per_label.append(list(i))\n",
    "        \n",
    "    image_paths_train = np.hstack(list(map(lambda one_class: one_class[:-10], image_paths_per_label)))\n",
    "    image_paths_test = np.hstack(list(map(lambda one_class: one_class[-10:], image_paths_per_label)))\n",
    "\n",
    "    trainset = pd.DataFrame({'image_path': image_paths_train})\n",
    "    testset  = pd.DataFrame({'image_path': image_paths_test })\n",
    "    trainset = trainset[ trainset['image_path'].map( lambda x: x.endswith('.jpg'))]\n",
    "    \n",
    "    #seperate each label\n",
    "    trainset['label'] = trainset['image_path'].map(lambda x: int(x.split('/')[-2].split('.')[0]) - 1)\n",
    "    trainset['label_name'] = trainset['image_path'].map(lambda x: x.split('/')[-2].split('.')[1])\n",
    "\n",
    "    testset = testset[ testset['image_path'].map( lambda x: x.endswith('.jpg'))]\n",
    "    testset['label'] = testset['image_path'].map(lambda x: int(x.split('/')[-2].split('.')[0]) - 1)\n",
    "    testset['label_name'] = testset['image_path'].map(lambda x: x.split('/')[-2].split('.')[1])\n",
    "\n",
    "    label_dict.to_pickle(label_dict_path)      #produce the labelset pickle file\n",
    "    trainset.to_pickle(trainset_path)          #produce the trainset pickle filee\n",
    "    testset.to_pickle(testset_path)            #produce the testset pickle filee\n",
    "    print ('Processed the dataset and pickled to disk: trainset, testset, and label_dict')\n",
    "else:\n",
    "    trainset = pd.read_pickle(trainset_path)\n",
    "    testset  = pd.read_pickle(testset_path)\n",
    "    label_dict = pd.read_pickle( label_dict_path)\n",
    "    print ('Read from disk: trainset, testset, and label_dict')\n",
    "    n_labels = len(label_dict)\n",
    "\n",
    "\n",
    "print ('Train set size: ' + str(len(trainset)))\n",
    "print ('Test set size: ' + str(len(testset)))\n",
    "trainset = trainset\n",
    "trainset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Epoch 1 Iteration 10\n",
      "Processed 540 / 28038\n",
      "Accuracy: 12 / 60\n",
      "Training Loss: 5.5095835\n",
      "======================================\n",
      "Epoch 1 Iteration 20\n",
      "Processed 1140 / 28038\n",
      "Accuracy: 22 / 60\n",
      "Training Loss: 4.323806\n",
      "======================================\n",
      "Epoch 1 Iteration 30\n",
      "Processed 1740 / 28038\n",
      "Accuracy: 26 / 60\n",
      "Training Loss: 3.4399712\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e33ccc87ca96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m                         \u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minit_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                         \u001b[0mimages_tf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcurrent_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                         \u001b[0mlabels_tf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcurrent_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                         })\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 1 # 10000    #times using all the training data traverse \n",
    "init_learning_rate = 0.01\n",
    "weight_decay_rate = 0.0005\n",
    "momentum = 0.9\n",
    "batch_size = 60\n",
    "max_iterations_per_epoch = -1 # use if you want to end the training prematurely (set to -1 to remove the max)\n",
    "\n",
    "now = datetime.now(pytz.timezone('US/Eastern'))\n",
    "seconds_since_epoch_start = time.mktime(now.timetuple())\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    learning_rate = tf.placeholder( tf.float32, [])   #learning rate\n",
    "    images_tf = tf.placeholder( tf.float32, [None, 224, 224, 3], name=\"images\")   #image placeholder\n",
    "    labels_tf = tf.placeholder( tf.int64, [None], name='labels')                  #label placeholder\n",
    "\n",
    "    detector = Detector(weight_path, n_labels)\n",
    "\n",
    "    p1,p2,p3,p4,conv5, conv6, gap, output = detector.inference(images_tf)         #return each conv\n",
    "    \n",
    "    #loss function(necessary to modify here into MSE Loss Function)\n",
    "    loss_tf = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_tf, logits=output))\n",
    "    '''\n",
    "    #OldTF\n",
    "    loss_tf = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits( output, labels_tf ))\n",
    "    '''\n",
    "\n",
    "    weights_only = filter( lambda x: x.name.endswith('W:0'), tf.trainable_variables() )\n",
    "    weight_decay = tf.reduce_sum(tf.stack([tf.nn.l2_loss(x) for x in weights_only])) * weight_decay_rate\n",
    "    '''\n",
    "    #OldTF\n",
    "    weight_decay = tf.reduce_sum(tf.pack([tf.nn.l2_loss(x) for x in weights_only])) * weight_decay_rate\n",
    "    '''\n",
    "\n",
    "    loss_tf += weight_decay   #update \n",
    "\n",
    "    saver = tf.train.Saver( max_to_keep=50 )\n",
    "\n",
    "    optimizer = tf.train.MomentumOptimizer( learning_rate, momentum )\n",
    "    grads_and_vars = optimizer.compute_gradients( loss_tf )\n",
    "    grads_and_vars = map(lambda gv: (gv[0], gv[1]) if ('conv6' in gv[1].name or 'GAP' in gv[1].name) else (gv[0]*0.1, gv[1]), grads_and_vars)\n",
    "    #grads_and_vars = [(tf.clip_by_value(gv[0], -5., 5.), gv[1]) for gv in grads_and_vars]\n",
    "    train_op = optimizer.apply_gradients( grads_and_vars )\n",
    "    \n",
    "with tf.Session(graph=graph) as sess:    \n",
    "    tf.global_variables_initializer().run()\n",
    "    '''\n",
    "    #OldTF\n",
    "    tf.initialize_all_variables().run()\n",
    "    '''\n",
    "\n",
    "    if pretrained_model:\n",
    "        print ('Pretrained model loaded from ' + pretrained_model + ' (this overwrites the initial weights loaded to the model)')\n",
    "        saver.restore(sess, pretrained_model)\n",
    "\n",
    "    testset.index  = range( len(testset) )\n",
    "\n",
    "    iterations = 0\n",
    "    loss_list = []\n",
    "    print ('Starting the training ...')\n",
    "    for epoch in range(n_epochs):\n",
    "        trainset.index = range( len(trainset) )\n",
    "        trainset = trainset.loc[ np.random.permutation( len(trainset) )]\n",
    "        '''\n",
    "        #OldPandas\n",
    "        trainset = trainset.ix[ np.random.permutation( len(trainset) )]\n",
    "        '''\n",
    "\n",
    "        for start, end in zip(\n",
    "            range( 0, len(trainset)+batch_size, batch_size),\n",
    "            range(batch_size, len(trainset)+batch_size, batch_size)):\n",
    "\n",
    "            current_data = trainset[start:end]\n",
    "            current_image_paths = current_data['image_path'].values\n",
    "            current_images = np.array(list(map(lambda x: load_image(x), current_image_paths)))\n",
    "\n",
    "            good_index = np.array(list(map(lambda x: x is not None, current_images)))\n",
    "\n",
    "            current_data = current_data[good_index]\n",
    "            current_images = np.stack(current_images[good_index])\n",
    "            current_labels = current_data['label'].values\n",
    "\n",
    "            _, loss_val, output_val = sess.run(\n",
    "                    [train_op, loss_tf, output],\n",
    "                    feed_dict={\n",
    "                        learning_rate: init_learning_rate,\n",
    "                        images_tf: current_images,\n",
    "                        labels_tf: current_labels\n",
    "                        })\n",
    "\n",
    "            loss_list.append( loss_val )\n",
    "\n",
    "            iterations += 1\n",
    "            if iterations % 10 == 0:\n",
    "                print (\"======================================\")\n",
    "                print (\"Epoch\", epoch + 1, \"Iteration\", iterations)\n",
    "                print (\"Processed\", start, '/', len(trainset))\n",
    "\n",
    "                label_predictions = output_val.argmax(axis=1)\n",
    "                acc = (label_predictions == current_labels).sum()\n",
    "\n",
    "                print (\"Accuracy:\", acc, '/', len(current_labels))\n",
    "                print (\"Training Loss:\", np.mean(loss_list))\n",
    "                #print \"\\n\"\n",
    "                loss_list = []\n",
    "\n",
    "            if iterations > max_iterations_per_epoch and max_iterations_per_epoch != -1:\n",
    "                iterations = 0\n",
    "                break\n",
    "\n",
    "        n_correct = 0\n",
    "        n_data = 0\n",
    "        for start, end in zip(\n",
    "                range(0, len(testset)+batch_size, batch_size),\n",
    "                range(batch_size, len(testset)+batch_size, batch_size)\n",
    "                ):\n",
    "            current_data = testset[start:end]\n",
    "            current_image_paths = current_data['image_path'].values\n",
    "            current_images = np.array(map(lambda x: load_image(x), current_image_paths))\n",
    "\n",
    "            good_index = np.array(map(lambda x: x is not None, current_images))\n",
    "\n",
    "            current_data = current_data[good_index]\n",
    "            current_images = np.stack(current_images[good_index])\n",
    "            current_labels = current_data['label'].values\n",
    "\n",
    "            output_vals = sess.run(\n",
    "                    output,\n",
    "                    feed_dict={images_tf:current_images})\n",
    "\n",
    "            label_predictions = output_vals.argmax(axis=1)\n",
    "            acc = (label_predictions == current_labels).sum()\n",
    "\n",
    "            n_correct += acc\n",
    "            n_data += len(current_data)\n",
    "\n",
    "        acc_all = n_correct / float(n_data)\n",
    "        print (\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "        print ('Model evaluation on the test set:')\n",
    "        print ('Epoch:'+str(epoch + 1)+'\\tacc:'+str(acc_all))\n",
    "        print (\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "\n",
    "        saver.save( sess, os.path.join( model_path, saved_model_name), global_step=epoch)\n",
    "\n",
    "        init_learning_rate *= 0.99\n",
    "    \n",
    "now = datetime.now(pytz.timezone('US/Eastern'))\n",
    "seconds_since_epoch_end = time.mktime(now.timetuple())\n",
    "print ('Processing took ' + str( np.around( (seconds_since_epoch_end - seconds_since_epoch_start)/60.0 , decimals=1) ) + ' minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing the Class Activation Map for the samples in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The passed save_path is not a valid checkpoint: /Users/chenguanghao/Desktop/Amanda/Weakly_detector/trained_models/VGG/model-1-1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-856635155dac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_model_name_testing\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     for start, end in zip(\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1536\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheckpoint_management\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m       raise ValueError(\"The passed save_path is not a valid checkpoint: \"\n\u001b[0;32m-> 1538\u001b[0;31m                        + compat.as_text(save_path))\n\u001b[0m\u001b[1;32m   1539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restoring parameters from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The passed save_path is not a valid checkpoint: /Users/chenguanghao/Desktop/Amanda/Weakly_detector/trained_models/VGG/model-1-1"
     ]
    }
   ],
   "source": [
    "# Reading the testset, creating the graph, and restoring the trained model:\n",
    "num_cam_images = 10 # Number of images randomly selected for which we are producing the CAM\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "testset = pd.read_pickle(testset_path )#[::-1][:20]\n",
    "label_dict = pd.read_pickle(label_dict_path)\n",
    "n_labels = len(label_dict)\n",
    "\n",
    "random_selection = np.random.randint(0, len(testset) - batch_size , size=num_cam_images)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    images_tf = tf.placeholder( tf.float32, [None, 224, 224, 3], name=\"images\")\n",
    "    labels_tf = tf.placeholder( tf.int64, [None], name='labels')\n",
    "\n",
    "    detector = Detector(weight_path,n_labels)\n",
    "    c1,c2,c3,c4,conv5, conv6, gap, output = detector.inference(images_tf )\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    classmap = detector.get_classmap( labels_tf, conv6 )\n",
    "\n",
    "with tf.Session(graph=graph) as sess:    \n",
    "    \n",
    "    saver.restore( sess, os.path.join( model_path, saved_model_name_testing) )\n",
    "\n",
    "    for start, end in zip(\n",
    "        random_selection,\n",
    "        list(random_selection + batch_size)):\n",
    "        '''\n",
    "        range( 0, len(testset)+batch_size, batch_size),\n",
    "        range(batch_size, len(testset)+batch_size, batch_size)):\n",
    "\n",
    "        if end >= num_cam_images:\n",
    "            break\n",
    "        '''\n",
    "\n",
    "        current_data = testset[start:end]\n",
    "        current_image_paths = current_data['image_path'].values\n",
    "        current_images = np.array(map(lambda x: load_image(x), current_image_paths))\n",
    "\n",
    "        good_index = np.array(map(lambda x: x is not None, current_images))\n",
    "\n",
    "        current_data = current_data[good_index]\n",
    "        current_image_paths = current_image_paths[good_index]\n",
    "        current_images = np.stack(current_images[good_index])\n",
    "        current_labels = current_data['label'].values\n",
    "        current_label_names = current_data['label_name'].values\n",
    "\n",
    "        conv6_val, output_val = sess.run(\n",
    "                [conv6, output],\n",
    "                feed_dict={\n",
    "                    images_tf: current_images\n",
    "                    })\n",
    "\n",
    "        label_predictions = output_val.argmax( axis=1 )\n",
    "        acc = (label_predictions == current_labels).sum()\n",
    "\n",
    "        classmap_vals = sess.run(\n",
    "                classmap,\n",
    "                feed_dict={\n",
    "                    labels_tf: label_predictions,\n",
    "                    conv6: conv6_val\n",
    "                    })\n",
    "\n",
    "        classmap_answer = sess.run(\n",
    "                classmap,\n",
    "                feed_dict={\n",
    "                    labels_tf: current_labels,\n",
    "                    conv6: conv6_val\n",
    "                    })\n",
    "\n",
    "        classmap_vis = map(lambda x: ((x-x.min())/(x.max()-x.min())), classmap_answer)\n",
    "\n",
    "        print (\"======================================\")\n",
    "        for vis, ori,ori_path, l_name in zip(classmap_vis, current_images, current_image_paths, current_label_names):\n",
    "            print (l_name)\n",
    "            plt.imshow( ori )\n",
    "            plt.imshow( vis, cmap=plt.cm.jet, alpha=0.5, interpolation='nearest' )\n",
    "            plt.show()\n",
    "\n",
    "            #vis_path = '/path/'+ ori_path.split('/')[-1]\n",
    "            #vis_path_ori = '/path/'+ori_path.split('/')[-1].split('.')[0]+'.ori.jpg'\n",
    "            #skimage.io.imsave( vis_path, vis )\n",
    "            #skimage.io.imsave( vis_path_ori, ori )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
